{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning project of the Udacity course \"Intro to Machine Learning\"\n",
    "The course is [here](https://www.udacity.com/course/intro-to-machine-learning--ud120), aiming at recognizing the pattern from the Enron emails dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## packages import\n",
    "import six.moves.cPickle as pickle\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import numpy as np\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### email author recognition\n",
    "The first interesting task using the Enron emails is a classification problem -- to identify who may sent the email (from Sara or Chris here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def parseOutText(f):\n",
    "    \"\"\" \n",
    "    Given an opened email file f, parse out all text below the\n",
    "    metadata block at the top, stem the words, and\n",
    "    return a string that contains all the stemmed words\n",
    "    in the email (space-separated)\n",
    "        \n",
    "    Example use case:\n",
    "    f = open(\"email_file_name.txt\", \"r\")\n",
    "    text = parseOutText(f)\n",
    "        \n",
    "    \"\"\"\n",
    "    f.seek(0)  # go back to beginning of file\n",
    "    all_text = f.read()\n",
    "\n",
    "    # split off metadata\n",
    "    content = all_text.split(\"X-FileName:\")\n",
    "    words = \"\"\n",
    "    if len(content) > 1:\n",
    "        # remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "        # split the text string into individual words, stem each word, and append the\n",
    "        # stemmed word to words (there's a single space between each stemmed word)\n",
    "        from nltk.stem.snowball import SnowballStemmer\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        words_list = text_string.split()\n",
    "        for word in words_list:\n",
    "            words = words + ' ' + stemmer.stem(word)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples in word_data: 17578\n",
      "Num of different words: 38755\n"
     ]
    }
   ],
   "source": [
    "word_file = './text_learning/your_word_data.pkl'\n",
    "author_file = './text_learning/your_email_authors.pkl'\n",
    "\n",
    "if not os.path.exists(word_file) or not os.path.exists(author_file):\n",
    "    ## Load and process email data\n",
    "    from_sara  = open(\"./text_learning/from_sara.txt\", \"r\")\n",
    "    from_chris = open(\"./text_learning/from_chris.txt\", \"r\")\n",
    "\n",
    "    authors = []  # to make labels from the email author\n",
    "    word_data = []  # email content (stemmed words) as the data\n",
    "\n",
    "    for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "        for path in from_person:\n",
    "            path = os.path.join('.', path[:-1])\n",
    "            print path\n",
    "            email = open(path, \"r\")\n",
    "\n",
    "            # use parseOutText to extract the text from the opened email\n",
    "            words = parseOutText(email)\n",
    "            \n",
    "            # sig is the list of signatures that can recognize the mail's author (machine learning thus useless)\n",
    "            # so we drop them\n",
    "            # sig can be extracted from an overfitted model by seeing the feature importances\n",
    "            # and overfitted model can be implemented from a small data set\n",
    "            sig = [\"sara\", \"shackleton\", \"chris\", \"germani\", \"sshacklensf\", \"cgermannsf\"]\n",
    "            for s in sig:\n",
    "                words = words.replace(s, \"\")\n",
    "            \n",
    "            # append the text to word_data\n",
    "            word_data.append(words)\n",
    "\n",
    "            # append a 0 to authors if email is from Sara, and 1 if email is from Chris\n",
    "            if name is \"sara\":\n",
    "                authors.append(0)\n",
    "            else:\n",
    "                authors.append(1)\n",
    "\n",
    "            email.close()\n",
    "\n",
    "    from_sara.close()\n",
    "    from_chris.close()\n",
    "\n",
    "    # save the processed data\n",
    "    pickle.dump( word_data, open(\"./text_learning/your_word_data.pkl\", \"w\") )\n",
    "    pickle.dump( authors, open(\"./text_learning/your_email_authors.pkl\", \"w\") )\n",
    "    print \"emails processed and saved.\"\n",
    "\n",
    "\n",
    "with open(\"./text_learning/your_word_data.pkl\") as f:\n",
    "    word_data = pickle.load(f)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "vectorizer.fit(word_data)\n",
    "features = vectorizer.get_feature_names()\n",
    "print \"Num of samples in word_data:\", len(word_data) # 17578 examples\n",
    "print \"Num of different words:\", len(features) # 38755 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: (15820, 876)\n",
      "test set size: (1758, 876)\n"
     ]
    }
   ],
   "source": [
    "## Reload the data to train ML models\n",
    "words_file = \"./text_learning/your_word_data.pkl\" \n",
    "authors_file = \"./text_learning/your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\") )\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, authors, \\\n",
    "    test_size=0.1, random_state=42)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.1, min_df=0.01, stop_words='english')\n",
    "trainset = vectorizer.fit_transform(features_train).toarray()\n",
    "testset  = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "print \"training set size:\", trainset.shape\n",
    "print \"test set size:\", testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes model evaluation is 95.6769055745%\n",
      "Logistic regression model evaluation is 97.1558589306%\n",
      "Adaboost model evaluation is 92.0932878271%\n",
      "Random forest model evaluation is 98.6348122867%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf.fit(trainset, labels_train)\n",
    "print \"Naive Bayes model evaluation is {0}%\".format(clf.score(testset, labels_test) * 100)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(trainset, labels_train)\n",
    "print \"Logistic regression model evaluation is {0}%\".format(clf.score(testset, labels_test) * 100)\n",
    "clf = AdaBoostClassifier()\n",
    "clf.fit(trainset, labels_train)\n",
    "print \"Adaboost model evaluation is {0}%\".format(clf.score(testset, labels_test) * 100)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(trainset, labels_train)\n",
    "print \"Random forest model evaluation is {0}%\".format(clf.score(testset, labels_test) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recognize the person of interest (POI) in the Enron scandal\n",
    "The main project is to identify Enron employees who may have committed fraud based on the public Enron financial and email dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" Given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "    \"\"\"\n",
    "    import math\n",
    "    poi_messages, all_messages = float(poi_messages), float(all_messages)\n",
    "    if math.isnan(poi_messages) or math.isnan(all_messages):\n",
    "        fraction = 0\n",
    "    else:\n",
    "        fraction = poi_messages / all_messages\n",
    "\n",
    "    return fraction\n",
    "\n",
    "\n",
    "def addFeature(data_dict):\n",
    "    \"\"\"Add the two features 'fraction_from_poi' and 'fraction_to_poi' \n",
    "       to the dataset\n",
    "    \"\"\"\n",
    "    for name in data_dict:\n",
    "        data_point = data_dict[name]\n",
    "\n",
    "        from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "        to_messages = data_point[\"to_messages\"]\n",
    "        fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "        data_dict[name][\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "        from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "        from_messages = data_point[\"from_messages\"]\n",
    "        fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "        data_dict[name][\"fraction_to_poi\"] = fraction_to_poi\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def featureFormat(dictionary, features, remove_NaN=True, remove_all_zeroes=True, remove_any_zeroes=False, sort_keys = False):\n",
    "    \"\"\" Convert dictionary to numpy array of features\n",
    "        remove_NaN = True will convert \"NaN\" string to 0.0\n",
    "        remove_all_zeroes = True will omit any data points for which\n",
    "            all the features you seek are 0.0\n",
    "        remove_any_zeroes = True will omit any data points for which\n",
    "            any of the features you seek are 0.0\n",
    "        sort_keys = True sorts keys by alphabetical order. Setting the value as\n",
    "            a string opens the corresponding pickle file with a preset key\n",
    "            order (this is used for Python 3 compatibility, and sort_keys\n",
    "            should be left as False for the course mini-projects).\n",
    "        NOTE: first feature is assumed to be 'poi' and is not checked for\n",
    "            removal for zero or missing values.\n",
    "    \"\"\"\n",
    "    return_list = []\n",
    "\n",
    "    # Key order - first branch is for Python 3 compatibility on mini-projects,\n",
    "    # second branch is for compatibility on final project.\n",
    "    if isinstance(sort_keys, str):\n",
    "        keys = pickle.load(open(sort_keys, \"rb\"))\n",
    "    elif sort_keys:\n",
    "        keys = sorted(dictionary.keys())\n",
    "    else:\n",
    "        keys = dictionary.keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tmp_list = []\n",
    "        for feature in features:\n",
    "            try:\n",
    "                dictionary[key][feature]\n",
    "            except KeyError:\n",
    "                print \"error: key \", feature, \" not present\"\n",
    "                return\n",
    "            value = dictionary[key][feature]\n",
    "            if value==\"NaN\" and remove_NaN:\n",
    "                value = 0\n",
    "            tmp_list.append( float(value) )\n",
    "\n",
    "        # Logic for deciding whether or not to add the data point.\n",
    "        append = True\n",
    "        # exclude 'poi' class as criteria.\n",
    "        if features[0] == 'poi':\n",
    "            test_list = tmp_list[1:]\n",
    "        else:\n",
    "            test_list = tmp_list\n",
    "        # if all features are zero and you want to remove\n",
    "        # data points that are all zero, default False\n",
    "        if remove_all_zeroes:\n",
    "            append = False\n",
    "            for item in test_list:\n",
    "                if item != 0 and item != \"NaN\":\n",
    "                    append = True\n",
    "                    break\n",
    "        # if any features for a given data point are zero and\n",
    "        # you want to remove data points with any zeroes, default False\n",
    "        if remove_any_zeroes:\n",
    "            if 0 in test_list or \"NaN\" in test_list:\n",
    "                append = False\n",
    "        # Append the data point if flagged for addition.\n",
    "        if append:\n",
    "            return_list.append( np.array(tmp_list) )\n",
    "\n",
    "    return np.array(return_list)\n",
    "\n",
    "\n",
    "def targetFeatureSplit( data ):\n",
    "    \"\"\" Given a numpy array like the one returned from\n",
    "        featureFormat, separate out the first feature\n",
    "        and put it into its own list, and return the\n",
    "        targets and features as separate lists\n",
    "    \"\"\"\n",
    "    target = []\n",
    "    features = []\n",
    "    for item in data:\n",
    "        target.append( item[0] )\n",
    "        features.append( item[1:] )\n",
    "\n",
    "    return target, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of features: 5\n",
      "Num of data points: 131\n"
     ]
    }
   ],
   "source": [
    "# Select the features of the dataset to be trained\n",
    "features_list = ['poi','fraction_to_poi','fraction_from_poi','shared_receipt_with_poi',\n",
    "                 'bonus','total_stock_value']\n",
    "\n",
    "with open(\"./final_project/final_project_dataset.pkl\", \"r\") as f:\n",
    "    my_dataset = pickle.load(f)\n",
    "\n",
    "my_dataset.pop('TOTAL')  # remove outliers\n",
    "my_dataset = addFeature(my_dataset)  # add my customed new features\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(features)\n",
    "features = scaler.transform(features)\n",
    "print \"Num of features:\", len(features_list)-1\n",
    "print \"Num of data points:\", len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "Now let's start training our models. Because the dataset is small, it's fast and handy to try several sklearn learning algorithms. And grid search cross-validation is used here to select the best hyper-parameters of each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper-parameters:\n",
      "{'C': 0.01}\n",
      "The test accuracy is 0.925925925926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "params = {'C':[.01, .1, 1, 10, 100]}\n",
    "cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "cvModel.fit(features_train, labels_train)\n",
    "print \"The best hyper-parameters:\\n  \", cvModel.best_params_\n",
    "print \"The test accuracy is\", cvModel.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper-parameters:\n",
      "{'kernel': 'linear', 'C': 0.01}\n",
      "The test accuracy is 0.925925925926\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC()\n",
    "params = {'kernel':('linear', 'rbf'), 'C':[.01, .1, 1, 10, 100]}\n",
    "cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "cvModel.fit(features_train, labels_train)\n",
    "print \"The best hyper-parameters:\\n  \", cvModel.best_params_\n",
    "print \"The test accuracy is\", cvModel.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper-parameters:\n",
      "{'n_neighbors': 5, 'weights': 'uniform'}\n",
      "The test accuracy is 0.851851851852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "params = {'n_neighbors':[3,5,7], 'weights': ('uniform', 'distance')}\n",
    "cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "cvModel.fit(features_train, labels_train)\n",
    "print \"The best hyper-parameters:\\n  \", cvModel.best_params_\n",
    "print \"The test accuracy is\", cvModel.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper-parameters:\n",
      "{'max_features': 4, 'max_depth': 5}\n",
      "The test accuracy is 0.814814814815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "params = {'max_features':[3,4,5], 'max_depth':[3,5,7]}\n",
    "cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "cvModel.fit(features_train, labels_train)\n",
    "print \"The best hyper-parameters:\\n  \", cvModel.best_params_\n",
    "print \"The test accuracy is\", cvModel.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best hyper-parameters:\n",
      "{'max_features': 2, 'n_estimators': 15, 'max_depth': 3}\n",
      "The test accuracy is 0.851851851852\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(random_state=45)\n",
    "params = {'n_estimators':[5, 10, 15], 'max_features':[2,4], 'max_depth':[None, 3, 5]}\n",
    "cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "cvModel.fit(features_train, labels_train)\n",
    "print \"The best hyper-parameters:\\n  \", cvModel.best_params_\n",
    "print \"The test accuracy is\", cvModel.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Average of different splitting\n",
    "To see the effect of random splitting of training set and test set, we try to see average of different splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg of test accuracy by SVM is 0.901234567901\n"
     ]
    }
   ],
   "source": [
    "state = [23, 35, 42, 51, 68, 99]\n",
    "\n",
    "accu_list = []\n",
    "for s in state:\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "        train_test_split(features, labels, test_size=0.2, random_state=s)\n",
    "    clf = SVC()\n",
    "    params = {'kernel':('linear', 'rbf'), 'C':[.01, .1, 1, 10, 100]}\n",
    "    cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "    cvModel.fit(features_train, labels_train)\n",
    "    accu_list.append(cvModel.score(features_test, labels_test))\n",
    "\n",
    "print \"The avg of test accuracy by SVM is\", np.mean(accu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The avg of test accuracy by kNN is 0.851851851852\n"
     ]
    }
   ],
   "source": [
    "accu_list = []\n",
    "for s in state:\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "        train_test_split(features, labels, test_size=0.2, random_state=s)\n",
    "    clf = KNeighborsClassifier()\n",
    "    params = {'n_neighbors':[3,5,7], 'weights': ('uniform', 'distance')}\n",
    "    cvModel = GridSearchCV(clf, param_grid=params, cv=5)\n",
    "    cvModel.fit(features_train, labels_train)\n",
    "    accu_list.append(cvModel.score(features_test, labels_test))\n",
    "\n",
    "print \"The avg of test accuracy by kNN is\", np.mean(accu_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
